from crawler import *
from pagerank import page_rank
import redis
import time

# Get crawler object and crawl on urls found in urls.txt
crawler = crawler(None, 'urls.txt')
start = time.time()
crawler.crawl()
print "Elapsed Time: %s" % (time.time() - start)
# Get the data structures generated by the crawler
lexicon = crawler.get_lexicon()
inverted_index = crawler.get_inverted_index()
resolved_inverted_index = crawler.get_resolved_inverted_index()
document_index = crawler.get_document_index()
# Run pagerank on the links generated by the crawler
pagerank = page_rank(crawler._links)
# Store data on persistent storage i.e. Redis
rdb = redis.Redis()
rdb.flushdb()

all_words = ''

for word in lexicon:
    rdb.set('lexicon:' + str(word), lexicon[word])
    all_words = all_words + str(word) + " , "

rdb.set('all_words', all_words.strip(' , '))

for word_id in inverted_index:
    rdb.set('inverted_index:' + str(word_id), str(list(inverted_index[word_id])).strip('[]'))
for word in resolved_inverted_index:
    rdb.set('resolved_inverted_index:' + str(word), str(list(resolved_inverted_index[word])).strip('[]'))
    rdb.set('num_urls:' + str(len(resolved_inverted_index[word])))
for doc_id in document_index:
    doc = document_index[doc_id]
    rdb.set('url:' + str(doc_id), doc[0])
    rdb.set('title:' + str(doc_id), doc[1])
    try:
    	rdb.set('description:' + str(doc_id), doc[2])
    except:
    	rdb.set('words:' + str(doc_id), doc[3])
for doc_id in pagerank:
    rdb.set('pagerank:' + str(doc_id), pagerank[doc_id])

rdb.save()
